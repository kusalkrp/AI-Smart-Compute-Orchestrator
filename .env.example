# AI Smart Compute Orchestrator — Environment Variables
# Copy this file to .env and fill in your values

# ─── Server ───────────────────────────────────────────────────────────────────
API_HOST=0.0.0.0
API_PORT=8000
API_KEYS=your-secret-key-1,your-secret-key-2
DEBUG=false

# ─── Infrastructure ───────────────────────────────────────────────────────────
REDIS_URL=redis://localhost:6379/0
DATABASE_URL=postgresql+asyncpg://orchestrator:orchestrator@localhost:5432/orchestrator

# ─── Ollama (Local Inference) ─────────────────────────────────────────────────
OLLAMA_BASE_URL=http://localhost:11434
GPU_MODEL=mistral:7b-instruct-q4_0
CPU_MODEL=phi3:mini
QUANTIZED_MODEL_PATH=./models/phi-3-mini-q4.gguf

# ─── Cloud LLM (Google Gemini) ────────────────────────────────────────────────
GEMINI_API_KEY=your-gemini-api-key-here
CLOUD_MODEL=gemini-2.0-flash

# ─── Routing Configuration ────────────────────────────────────────────────────
ROUTING_STAGE=rule
# Options: rule | scored | ml
ML_MODEL_PATH=./models/routing_model.pkl
ML_CONFIDENCE_THRESHOLD=0.6
ROUTING_POLICY_PATH=./config/routing_policy.yaml
COST_CONFIG_PATH=./config/cost_config.yaml

# ─── Resource Thresholds ──────────────────────────────────────────────────────
GPU_OVERLOAD_PERCENT=85.0
CPU_OVERLOAD_PERCENT=90.0
GPU_VRAM_BUFFER_MB=512

# ─── Cost Configuration ───────────────────────────────────────────────────────
# gemini-2.0-flash avg: $0.10/1M input + $0.40/1M output → $0.00025/1k tokens
CLOUD_COST_PER_1K_TOKENS=0.00025
LOCAL_COMPUTE_COST_PER_HOUR=0.0

# ─── Celery ───────────────────────────────────────────────────────────────────
CELERY_BROKER_URL=redis://localhost:6379/1
CELERY_RESULT_BACKEND=redis://localhost:6379/2

# ─── Monitoring ───────────────────────────────────────────────────────────────
RESOURCE_MONITOR_INTERVAL_SEC=2
RESOURCE_SNAPSHOT_DB_INTERVAL_SEC=30

# ─── Dashboard ────────────────────────────────────────────────────────────────
DASHBOARD_API_BASE_URL=http://localhost:8000
